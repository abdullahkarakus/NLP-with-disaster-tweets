{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't be using keyword, location, and id information in our models so we drop them. We need to retain the id column of test_data since we will be using it to submit our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.drop(columns = [\"keyword\",\"location\",\"id\"])\n",
    "id_column = test_data[\"id\"].copy()\n",
    "test_data = test_data.drop(columns = [\"keyword\",\"location\",\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    7613 non-null   object\n",
      " 1   target  7613 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 119.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    3263 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 25.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(training_data.info())\n",
    "display(test_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no null-values. So we can proceed. Let's see some of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goulburn man Henry Van Bilsen missing: Emergency services are searching for a Goulburn man who disappeared from hisÛ_ http://t.co/z99pKJzTRp\n",
      "The things we fear most in organizations--fluctuations disturbances imbalances--are the primary sources of creativity. - Margaret Wheatley\n",
      "@tsunami_esh ?? hey Esh\n",
      "@POTUS you until you drown by water entering the lungs. You being alive has caused this great country to fall to shit because you're a pussy\n",
      "Crawling in my skin\n",
      "These wounds they will not hea\n",
      "#np agalloch - the desolation song\n",
      "Hollywood Movie About Trapped Miners Released in Chile: 'The 33' Hollywood movie about trapped miners starring... http://t.co/tyyfG4qQvM\n",
      "New roof and hardy up..Windstorm inspection tomorrow http://t.co/kKeH8qCgc3\n",
      "The Catastrophic Effects of Hiroshima and Nagasaki Atomic Bombings Still Being Felt Today http://t.co/WC8AqXeDF7\n",
      "tiffanyfrizzell has a crush: http://t.co/RaF732vRtt\n",
      "Holy fuck QVC bitch just got burned so hard.\n",
      "I added a video to a @YouTube playlist http://t.co/vxeGCmMVBV Panic! At The Disco: Collar Full (Audio)\n",
      "@BehindAShield @Wars_Goddess Sweet Lord.  (I collapse as my knees buckle)\n",
      "A Laois girl advertised for a new friend to replace her loved-up BFF and has been inundated http://t.co/IGM2fc4T0M http://t.co/YiLTu7SXAr\n",
      "#WakeUpFlorida... #Floridians more likely to be killed/injured by a #TrophyHunt killer's gun than by ISIS.  https://t.co/j5In8meXAJ\n"
     ]
    }
   ],
   "source": [
    "for tweet in training_data[\"text\"].sample(15,random_state = 1):\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to remove #,@ symbols, quotation marks, punctuations, links, and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [training_data,test_data]\n",
    "\n",
    "for data in dataset:\n",
    "    data[\"text\"] = data[\"text\"].apply(lambda x: x.lower())\n",
    "    data[\"text\"] = data[\"text\"].apply(lambda x: re.sub(\"(@|#)\",\"\",x))\n",
    "    data[\"text\"] = data[\"text\"].apply(lambda x: re.sub(\"http[^\\s]*\",\"\",x))\n",
    "    data[\"text\"] = data[\"text\"].apply(lambda x: re.sub(\"[0-9]*\",\"\",x))\n",
    "    data[\"text\"] = data[\"text\"].apply(lambda x: re.sub(\"[^a-z]+\",\" \",x))\n",
    "    data[\"text\"] = data[\"text\"].apply(lambda x: re.sub(\"(?<=\\s)[a-z](?=\\s)\",\"\",x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goulburn man henry van bilsen missing emergency services are searching for  goulburn man who disappeared from his \n",
      "the things we fear most in organizations fluctuations disturbances imbalances are the primary sources of creativity margaret wheatley\n",
      "tsunami esh hey esh\n",
      "potus you until you drown by water entering the lungs you being alive has caused this great country to fall to shit because you re  pussy\n",
      "crawling in my skin these wounds they will not hea\n",
      "np agalloch the desolation song\n",
      "hollywood movie about trapped miners released in chile the hollywood movie about trapped miners starring \n",
      "new roof and hardy up windstorm inspection tomorrow \n",
      "the catastrophic effects of hiroshima and nagasaki atomic bombings still being felt today \n",
      "tiffanyfrizzell has  crush \n",
      "holy fuck qvc bitch just got burned so hard \n",
      "i added  video to  youtube playlist panic at the disco collar full audio \n",
      "behindashield wars goddess sweet lord  collapse as my knees buckle \n",
      "a laois girl advertised for  new friend to replace her loved up bff and has been inundated \n",
      "wakeupflorida floridians more likely to be killed injured by  trophyhunt killer  gun than by isis \n"
     ]
    }
   ],
   "source": [
    "for tweet in training_data[\"text\"].sample(15,random_state = 1):\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will tokenize each tweet so that we can remove stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['our', 'deeds', 'are', 'the', 'reason', 'of', 'this', 'earthquake', 'may', 'allah', 'forgive', 'us', 'all']\n",
      "['forest', 'fire', 'near', 'la', 'ronge', 'sask', 'canada']\n",
      "['all', 'residents', 'asked', 'to', 'shelter', 'in', 'place', 'are', 'being', 'notified', 'by', 'officers', 'no', 'other', 'evacuation', 'or', 'shelter', 'in', 'place', 'orders', 'are', 'expected']\n",
      "['people', 'receive', 'wildfires', 'evacuation', 'orders', 'in', 'california']\n",
      "['just', 'got', 'sent', 'this', 'photo', 'from', 'ruby', 'alaska', 'as', 'smoke', 'from', 'wildfires', 'pours', 'into', 'school']\n"
     ]
    }
   ],
   "source": [
    "for data in dataset:\n",
    "    data[\"text\"] = data[\"text\"].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "for tweet in training_data[\"text\"][0:5]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "for data in dataset:\n",
    "    data[\"text\"] = data[\"text\"].apply(lambda x: [token for token in x if not token in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will lemmatize each tweet using the grammatical class of each word so that we can reduce them to their roots. For each word we will find which part of speech it belongs to using pos_tag function and then use this information while lemmatizing words. We also need to define a function that will turn the pos information into one that can be used by the lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for data in dataset:\n",
    "    data[\"text\"] = data[\"text\"].apply(lambda x: [lemmatizer.lemmatize(token,get_wordnet_pos(token)) for token in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deed', 'reason', 'earthquake', 'may', 'allah', 'forgive', 'u']\n",
      "['forest', 'fire', 'near', 'la', 'ronge', 'sask', 'canada']\n",
      "['resident', 'ask', 'shelter', 'place', 'notify', 'officer', 'evacuation', 'shelter', 'place', 'order', 'expect']\n",
      "['people', 'receive', 'wildfire', 'evacuation', 'order', 'california']\n",
      "['get', 'sent', 'photo', 'ruby', 'alaska', 'smoke', 'wildfire', 'pours', 'school']\n"
     ]
    }
   ],
   "source": [
    "for tweet in training_data[\"text\"][0:5]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tweet is turned into a list of words. Now we need to turn the list into a string again so we can use CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataset:\n",
    "    data[\"text\"] = data[\"text\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deed reason earthquake may allah forgive u\n",
      "forest fire near la ronge sask canada\n",
      "resident ask shelter place notify officer evacuation shelter place order expect\n",
      "people receive wildfire evacuation order california\n",
      "get sent photo ruby alaska smoke wildfire pours school\n"
     ]
    }
   ],
   "source": [
    "for tweet in training_data[\"text\"][0:5]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving ahead with vectorizer, we will first split our training_data into two parts: training and testing. This is so that we can try multiple machine learning models and decide which one performs better. The test_data doesn't contain the target values; it will be used only for the final prediction, to be submitted to kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(training_data[\"text\"], training_data[\"target\"], test_size = 0.2\n",
    "                                                   , random_state = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create vectors using all the words, i.e. min_df = 1, and using a one-gram model, i.e ngram_range = 1. Later we will try to optimize our model varying these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df = 1, ngram_range = (1,1))\n",
    "X_train = vectorizer.fit_transform(X_train).toarray()\n",
    "X_test = vectorizer.transform(X_test).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code gives us the vocabulary used to create the vectors. There are 12133 words. Increasing min_df would decrease the number of words available since most of these words are not meaningful, they are caused by spelling mistakes and they probably appear once throughout the entire tweet collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12133"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = vectorizer.get_feature_names_out()\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "aaaaaaallll\n",
      "aaarrrgghhh\n",
      "aace\n",
      "aal\n",
      "aamir\n",
      "aan\n",
      "aannnnd\n",
      "aar\n",
      "ab\n",
      "aba\n",
      "abandon\n",
      "abandonedpics\n",
      "abbandoned\n",
      "abbott\n",
      "abbruchsimulator\n",
      "abbswinston\n",
      "abbyairshow\n",
      "abc\n",
      "abcchicago\n",
      "abceyewitness\n",
      "abcnews\n",
      "abcnorio\n",
      "abe\n",
      "aberdeen\n"
     ]
    }
   ],
   "source": [
    "for word in features[0:25]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try Logistic Regression and Bernoulli Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state = 30)\n",
    "LR.fit(X_train,y_train)\n",
    "acc_LR = LR.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7931713722915299"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8036769533814839"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bernoulli_nb = BernoulliNB()\n",
    "bernoulli_nb.fit(X_train,y_train)\n",
    "acc_bnb = bernoulli_nb.score(X_test,y_test) \n",
    "\n",
    "acc_bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for n_grams = (1, 1) and min_df = 1, the LR acc is 0.7931713722915299 and Bernoulli NB acc is 0.8036769533814839.\n",
      "for n_grams = (1, 1) and min_df = 2, the LR acc is 0.7872619829284307 and Bernoulli NB acc is 0.8023637557452397.\n",
      "for n_grams = (1, 1) and min_df = 3, the LR acc is 0.7892317793827971 and Bernoulli NB acc is 0.804333552199606.\n",
      "for n_grams = (1, 1) and min_df = 4, the LR acc is 0.7898883782009193 and Bernoulli NB acc is 0.8049901510177282.\n",
      "for n_grams = (1, 1) and min_df = 5, the LR acc is 0.7925147734734077 and Bernoulli NB acc is 0.7997373604727511.\n",
      "for n_grams = (1, 1) and min_df = 6, the LR acc is 0.7892317793827971 and Bernoulli NB acc is 0.7977675640183848.\n",
      "for n_grams = (1, 1) and min_df = 7, the LR acc is 0.7872619829284307 and Bernoulli NB acc is 0.7951411687458962.\n",
      "for n_grams = (1, 1) and min_df = 8, the LR acc is 0.7918581746552856 and Bernoulli NB acc is 0.7964543663821405.\n",
      "for n_grams = (1, 1) and min_df = 9, the LR acc is 0.7905449770190414 and Bernoulli NB acc is 0.7951411687458962.\n",
      "for n_grams = (1, 1) and min_df = 10, the LR acc is 0.7898883782009193 and Bernoulli NB acc is 0.7944845699277742.\n",
      "for n_grams = (2, 2) and min_df = 1, the LR acc is 0.7518056467498359 and Bernoulli NB acc is 0.7170059093893631.\n",
      "for n_grams = (2, 2) and min_df = 2, the LR acc is 0.7445830597504924 and Bernoulli NB acc is 0.7288246881155613.\n",
      "for n_grams = (2, 2) and min_df = 3, the LR acc is 0.7367038739330269 and Bernoulli NB acc is 0.7202889034799738.\n",
      "for n_grams = (2, 2) and min_df = 4, the LR acc is 0.7189757058437295 and Bernoulli NB acc is 0.7143795141168746.\n",
      "for n_grams = (2, 2) and min_df = 5, the LR acc is 0.711096520026264 and Bernoulli NB acc is 0.706500328299409.\n",
      "for n_grams = (2, 2) and min_df = 6, the LR acc is 0.7084701247537755 and Bernoulli NB acc is 0.7032173342087984.\n",
      "for n_grams = (2, 2) and min_df = 7, the LR acc is 0.7019041365725541 and Bernoulli NB acc is 0.6966513460275772.\n",
      "for n_grams = (2, 2) and min_df = 8, the LR acc is 0.695994747209455 and Bernoulli NB acc is 0.6946815495732108.\n",
      "for n_grams = (2, 2) and min_df = 9, the LR acc is 0.695994747209455 and Bernoulli NB acc is 0.6966513460275772.\n",
      "for n_grams = (2, 2) and min_df = 10, the LR acc is 0.6900853578463558 and Bernoulli NB acc is 0.6920551543007223.\n",
      "for n_grams = (3, 3) and min_df = 1, the LR acc is 0.7058437294812869 and Bernoulli NB acc is 0.685489166119501.\n",
      "for n_grams = (3, 3) and min_df = 2, the LR acc is 0.685489166119501 and Bernoulli NB acc is 0.6769533814839134.\n",
      "for n_grams = (3, 3) and min_df = 3, the LR acc is 0.6684175968483257 and Bernoulli NB acc is 0.66973079448457.\n",
      "for n_grams = (3, 3) and min_df = 4, the LR acc is 0.6539724228496389 and Bernoulli NB acc is 0.6533158240315168.\n",
      "for n_grams = (3, 3) and min_df = 5, the LR acc is 0.6460932370321734 and Bernoulli NB acc is 0.6454366382140512.\n",
      "for n_grams = (3, 3) and min_df = 6, the LR acc is 0.6428102429415627 and Bernoulli NB acc is 0.6428102429415627.\n",
      "for n_grams = (3, 3) and min_df = 7, the LR acc is 0.6421536441234406 and Bernoulli NB acc is 0.6428102429415627.\n",
      "for n_grams = (3, 3) and min_df = 8, the LR acc is 0.6421536441234406 and Bernoulli NB acc is 0.6428102429415627.\n",
      "for n_grams = (3, 3) and min_df = 9, the LR acc is 0.6375574523965857 and Bernoulli NB acc is 0.6382140512147079.\n",
      "for n_grams = (3, 3) and min_df = 10, the LR acc is 0.6296782665791202 and Bernoulli NB acc is 0.6303348653972423.\n",
      "for n_grams = (1, 2) and min_df = 1, the LR acc is 0.8017071569271176 and Bernoulli NB acc is 0.7806959947472094.\n",
      "for n_grams = (1, 2) and min_df = 2, the LR acc is 0.7944845699277742 and Bernoulli NB acc is 0.8030203545633617.\n",
      "for n_grams = (1, 2) and min_df = 3, the LR acc is 0.799080761654629 and Bernoulli NB acc is 0.804333552199606.\n",
      "for n_grams = (1, 2) and min_df = 4, the LR acc is 0.8023637557452397 and Bernoulli NB acc is 0.8003939592908733.\n",
      "for n_grams = (1, 2) and min_df = 5, the LR acc is 0.7997373604727511 and Bernoulli NB acc is 0.7957977675640184.\n",
      "for n_grams = (1, 2) and min_df = 6, the LR acc is 0.7944845699277742 and Bernoulli NB acc is 0.7957977675640184.\n",
      "for n_grams = (1, 2) and min_df = 7, the LR acc is 0.7918581746552856 and Bernoulli NB acc is 0.7912015758371634.\n",
      "for n_grams = (1, 2) and min_df = 8, the LR acc is 0.7951411687458962 and Bernoulli NB acc is 0.7912015758371634.\n",
      "for n_grams = (1, 2) and min_df = 9, the LR acc is 0.7977675640183848 and Bernoulli NB acc is 0.7931713722915299.\n",
      "for n_grams = (1, 2) and min_df = 10, the LR acc is 0.793827971109652 and Bernoulli NB acc is 0.7931713722915299.\n",
      "for n_grams = (1, 3) and min_df = 1, the LR acc is 0.804333552199606 and Bernoulli NB acc is 0.7485226526592252.\n",
      "for n_grams = (1, 3) and min_df = 2, the LR acc is 0.7977675640183848 and Bernoulli NB acc is 0.778069599474721.\n",
      "for n_grams = (1, 3) and min_df = 3, the LR acc is 0.7984241628365069 and Bernoulli NB acc is 0.788575180564675.\n",
      "for n_grams = (1, 3) and min_df = 4, the LR acc is 0.8030203545633617 and Bernoulli NB acc is 0.7964543663821405.\n",
      "for n_grams = (1, 3) and min_df = 5, the LR acc is 0.7984241628365069 and Bernoulli NB acc is 0.7925147734734077.\n",
      "for n_grams = (1, 3) and min_df = 6, the LR acc is 0.7944845699277742 and Bernoulli NB acc is 0.7918581746552856.\n",
      "for n_grams = (1, 3) and min_df = 7, the LR acc is 0.7918581746552856 and Bernoulli NB acc is 0.7905449770190414.\n",
      "for n_grams = (1, 3) and min_df = 8, the LR acc is 0.793827971109652 and Bernoulli NB acc is 0.7898883782009193.\n",
      "for n_grams = (1, 3) and min_df = 9, the LR acc is 0.7984241628365069 and Bernoulli NB acc is 0.7866053841103086.\n",
      "for n_grams = (1, 3) and min_df = 10, the LR acc is 0.7931713722915299 and Bernoulli NB acc is 0.7846355876559422.\n",
      "for n_grams = (2, 3) and min_df = 1, the LR acc is 0.7445830597504924 and Bernoulli NB acc is 0.6999343401181878.\n",
      "for n_grams = (2, 3) and min_df = 2, the LR acc is 0.7452396585686146 and Bernoulli NB acc is 0.7038739330269206.\n",
      "for n_grams = (2, 3) and min_df = 3, the LR acc is 0.7367038739330269 and Bernoulli NB acc is 0.695994747209455.\n",
      "for n_grams = (2, 3) and min_df = 4, the LR acc is 0.7222586999343401 and Bernoulli NB acc is 0.6940249507550886.\n",
      "for n_grams = (2, 3) and min_df = 5, the LR acc is 0.7137229152987524 and Bernoulli NB acc is 0.6953381483913329.\n",
      "for n_grams = (2, 3) and min_df = 6, the LR acc is 0.7091267235718975 and Bernoulli NB acc is 0.6920551543007223.\n",
      "for n_grams = (2, 3) and min_df = 7, the LR acc is 0.7019041365725541 and Bernoulli NB acc is 0.6881155613919895.\n",
      "for n_grams = (2, 3) and min_df = 8, the LR acc is 0.695994747209455 and Bernoulli NB acc is 0.6868023637557452.\n",
      "for n_grams = (2, 3) and min_df = 9, the LR acc is 0.695994747209455 and Bernoulli NB acc is 0.6881155613919895.\n",
      "for n_grams = (2, 3) and min_df = 10, the LR acc is 0.6900853578463558 and Bernoulli NB acc is 0.685489166119501.\n"
     ]
    }
   ],
   "source": [
    "n_grams = [(1,1),(1,2),(1,3)]\n",
    "min_df = [1,2,3,4,5]\n",
    "\n",
    "for gram in n_grams:\n",
    "    for df in min_df:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(training_data[\"text\"], training_data[\"target\"], test_size = 0.2\n",
    "                                                   , random_state = 12)\n",
    "        \n",
    "        vectorizer = CountVectorizer(min_df = df, ngram_range = gram)\n",
    "        X_train = vectorizer.fit_transform(X_train).toarray()\n",
    "        X_test = vectorizer.transform(X_test).toarray()\n",
    "        \n",
    "        LR = LogisticRegression(random_state = 30)\n",
    "        LR.fit(X_train,y_train)\n",
    "        acc_LR = LR.score(X_test,y_test)\n",
    "        \n",
    "        bernoulli_nb = BernoulliNB()\n",
    "        bernoulli_nb.fit(X_train,y_train)\n",
    "        acc_bnb = bernoulli_nb.score(X_test,y_test) \n",
    "        \n",
    "        print(\"for n_grams = \" + str(gram) + \" and min_df = \" + str(df) + \", the LR acc is \" + str(acc_LR) +\n",
    "              \" and \" + \"Bernoulli NB acc is \" + str(acc_bnb) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of just taking into account the existence/non-existence of words from the vocabulary in each tweet we can also take into account their frequencies within a tweet and accros the collection of tweets. This is provided by Tfidf Vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(training_data[\"text\"], training_data[\"target\"], test_size = 0.2\n",
    "                                                   , random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(min_df = 1, ngram_range = (1,1))\n",
    "X_train = vectorizer_tfidf.fit_transform(X_train).toarray()\n",
    "X_test = vectorizer_tfidf.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we try Logistic Regression and Multinomial Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7925147734734077"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR = LogisticRegression(random_state = 30)\n",
    "LR.fit(X_train,y_train)\n",
    "acc_LR = LR.score(X_test,y_test)\n",
    "\n",
    "acc_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7964543663821405"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_nb = MultinomialNB()\n",
    "multinomial_nb.fit(X_train,y_train)\n",
    "acc_mnb = multinomial_nb.score(X_test,y_test) \n",
    "\n",
    "acc_mnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for n_grams = (1, 1) and min_df = 1, the LR acc is 0.7925147734734077 and Multinomial NB acc is 0.7964543663821405.\n",
      "for n_grams = (1, 1) and min_df = 2, the LR acc is 0.7925147734734077 and Multinomial NB acc is 0.7964543663821405.\n",
      "for n_grams = (1, 1) and min_df = 3, the LR acc is 0.7925147734734077 and Multinomial NB acc is 0.7964543663821405.\n",
      "for n_grams = (1, 1) and min_df = 4, the LR acc is 0.7925147734734077 and Multinomial NB acc is 0.7964543663821405.\n",
      "for n_grams = (1, 1) and min_df = 5, the LR acc is 0.7925147734734077 and Multinomial NB acc is 0.7964543663821405.\n",
      "for n_grams = (1, 2) and min_df = 1, the LR acc is 0.7925147734734077 and Multinomial NB acc is 0.7964543663821405.\n",
      "for n_grams = (1, 2) and min_df = 2, the LR acc is 0.7925147734734077 and Multinomial NB acc is 0.7964543663821405.\n",
      "for n_grams = (1, 2) and min_df = 3, the LR acc is 0.7925147734734077 and Multinomial NB acc is 0.7964543663821405.\n",
      "for n_grams = (1, 2) and min_df = 4, the LR acc is 0.7925147734734077 and Multinomial NB acc is 0.7964543663821405.\n",
      "for n_grams = (1, 2) and min_df = 5, the LR acc is 0.7925147734734077 and Multinomial NB acc is 0.7964543663821405.\n",
      "for n_grams = (1, 3) and min_df = 1, the LR acc is 0.7925147734734077 and Multinomial NB acc is 0.7964543663821405.\n",
      "for n_grams = (1, 3) and min_df = 2, the LR acc is 0.7925147734734077 and Multinomial NB acc is 0.7964543663821405.\n",
      "for n_grams = (1, 3) and min_df = 3, the LR acc is 0.7925147734734077 and Multinomial NB acc is 0.7964543663821405.\n",
      "for n_grams = (1, 3) and min_df = 4, the LR acc is 0.7925147734734077 and Multinomial NB acc is 0.7964543663821405.\n",
      "for n_grams = (1, 3) and min_df = 5, the LR acc is 0.7925147734734077 and Multinomial NB acc is 0.7964543663821405.\n"
     ]
    }
   ],
   "source": [
    "n_grams = [(1,1),(1,2),(1,3)]\n",
    "min_df = [1,2,3,4,5]\n",
    "\n",
    "for gram in n_grams:\n",
    "    for df in min_df:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(training_data[\"text\"], training_data[\"target\"], test_size = 0.2\n",
    "                                                   , random_state = 12)\n",
    "        \n",
    "        vectorizer_tfidf = TfidfVectorizer(min_df = 1, ngram_range = (1,1))\n",
    "        X_train = vectorizer_tfidf.fit_transform(X_train).toarray()\n",
    "        X_test = vectorizer_tfidf.transform(X_test).toarray()\n",
    "        \n",
    "        LR = LogisticRegression(random_state = 30)\n",
    "        LR.fit(X_train,y_train)\n",
    "        acc_LR = LR.score(X_test,y_test)\n",
    "        \n",
    "        multinomial_nb = MultinomialNB()\n",
    "        multinomial_nb.fit(X_train,y_train)\n",
    "        acc_mnb = multinomial_nb.score(X_test,y_test) \n",
    "\n",
    "        acc_mnb\n",
    "        \n",
    "        print(\"for n_grams = \" + str(gram) + \" and min_df = \" + str(df) + \", the LR acc is \" + str(acc_LR) +\n",
    "              \" and \" + \"Multinomial NB acc is \" + str(acc_mnb) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding of vectors don't take into account the similarity of words. In this construction every word has the same distance to every other word. However, we would want words that have similar meanings to be close together. This is provided by Word2Vec function. To use this function we must first create a list of lists where each inner list contains one tweet in a tokenized form. Then we feed this list into Word2Vec function which creates the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_train, w2v_test, y_train, y_test = train_test_split(training_data[\"text\"], training_data[\"target\"], \n",
    "                                                        test_size = 0.2, random_state = 12)\n",
    "\n",
    "\n",
    "w2v_train = w2v_train.apply(lambda x: word_tokenize(x))\n",
    "w2v_test = w2v_test.apply(lambda x: word_tokenize(x))\n",
    "\n",
    "w2v_train.reset_index(drop = True, inplace = True)\n",
    "w2v_test.reset_index(drop = True, inplace = True)\n",
    "y_train.reset_index(drop = True, inplace = True)\n",
    "y_test.reset_index(drop = True, inplace = True)\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for token_list in w2v_train:\n",
    "    corpus.append(token_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_model = Word2Vec(sentences = corpus, vector_size = 200, window = 3, min_count = 4, seed = 42, epochs =50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec creates a vector for each word and not for each tweet. Therefore we must find a way to turn them into vectors for tweets. One thing we can do is we can take the average of each vector in the tweet. The following function does exactly that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(token_list,size):\n",
    "    vec = np.zeros(size).reshape((1,size))\n",
    "    count = 0\n",
    "    for token in token_list:\n",
    "        try:\n",
    "            vec += vector_model.wv[token].reshape((1,size))\n",
    "            count += 1.\n",
    "        except KeyError:  # handling the case where the token is not in vocabulary\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((len(w2v_train),200))\n",
    "X_test = np.zeros((len(w2v_test),200))\n",
    "\n",
    "for index in range(len(w2v_train)):\n",
    "    X_train[index,:] = word_vector(w2v_train[index],200)\n",
    "\n",
    "for index in range(len(w2v_test)):\n",
    "    X_test[index,:] = word_vector(w2v_test[index],200)\n",
    "    \n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "X_test = StandardScaler().fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state = 30, solver = \"newton-cg\")\n",
    "LR.fit(X_train,y_train)\n",
    "acc_LR = LR.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7590282337491793"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our copus is very limited, word2vec doesn't perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from above that we will create our vectors using Vectorizer with a mix of unigrams and bigrams. We shall exclude terms that appear less than 4 times. We choose Naive Bayse over Logistic Regression and train it using all the training data. Finally we predict labels of tweets in the test data and submit our predictions to kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df = 4, ngram_range = (1,2))\n",
    "X_train = vectorizer.fit_transform(training_data[\"text\"]).toarray()\n",
    "X_test = vectorizer.transform(test_data[\"text\"]).toarray()\n",
    "y_train = training_data[\"target\"]\n",
    "\n",
    "\n",
    "LR = LogisticRegression(random_state = 30)\n",
    "LR.fit(X_train,y_train)\n",
    "predictions = LR.predict(X_test)\n",
    "\n",
    "predictions = pd.Series(predictions)\n",
    "submit = pd.concat([id_column,predictions], axis = 1, keys =[\"id\", \"Target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv(\"NLP_with_disaster_tweets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
